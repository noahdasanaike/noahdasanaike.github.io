<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <title>socOCRbench - Noah Dasanaike</title>
  <style>
    body {
      max-width: 1000px;
      margin: auto;
      padding: 1em;
    }

    a {
      color: #750014;
    }

    .left-panel {
      float: left;
      width: 200px;
      padding-right: 2em;
    }

    .right-panel {
      margin-left: 250px;
    }

    .left-panel img {
      display: block;
      margin-bottom: 1em;
    }

    .left-panel h1 {
      margin: 0 0 1em 0;
    }

    h2,
    h3 {
      margin: 1em 0 0.5em 0;
      margin-left: 0;
    }

    ul {
      margin: 0 0 1em 0;
      margin-left: 0;
      padding-left: 2em;
    }

    li {
      margin: 0;
      padding: 0;
    }

    .post-content p {
      margin: 1em 0;
    }

    .post-date {
      color: #666;
      font-style: italic;
      margin-bottom: 1em;
    }

    .bench-title {
      text-align: center;
      font-size: 3em;
      font-weight: bold;
      margin: 0.8em 0 0.3em 0;
      letter-spacing: 0.02em;
    }

    .bench-subtitle {
      text-align: center;
      color: #666;
      font-size: 1.1em;
      margin-bottom: 1.5em;
    }

    .results-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 1.5em 0;
    }

    .results-table th,
    .results-table td {
      border: 1px solid #ddd;
      padding: 5px 8px;
      text-align: center;
    }

    .results-table th {
      background: #f5f5f5;
      font-weight: bold;
    }

    .results-table.sortable th {
      cursor: pointer;
      user-select: none;
    }

    .results-table.sortable th.sort-asc::after {
      content: " ^";
      font-size: 0.9em;
      color: #666;
    }

    .results-table.sortable th.sort-desc::after {
      content: " v";
      font-size: 0.9em;
      color: #666;
    }

    .results-table td:first-child {
      text-align: left;
      font-weight: 500;
    }

    .results-table tr:hover {
      background: #fafafa;
    }

    .best {
      font-weight: bold;
      color: #750014;
    }

    .model-tag {
      display: inline-block;
      margin-left: 0.5em;
      padding: 0.1em 0.5em;
      border-radius: 999px;
      font-size: 0.75em;
      font-weight: 600;
      letter-spacing: 0.02em;
      vertical-align: baseline;
      border: 1px solid #ddd;
      color: #444;
      background: #f7f7f7;
      white-space: nowrap;
    }

    .model-tag.vlm {
      border-color: #d8c7cc;
      background: #fbf4f6;
      color: #750014;
    }

    .model-tag.classical {
      border-color: #c7d6e2;
      background: #f2f7fb;
      color: #1b4f72;
    }

    .model-tag.open-source {
      border-color: #b8d4b8;
      background: #f0f8f0;
      color: #2d6a2d;
    }

    .model-tag.proprietary {
      border-color: #d4c8b8;
      background: #faf6f0;
      color: #6a5a2d;
    }

    .comments-section {
      margin-top: 3em;
      padding-top: 2em;
      border-top: 1px solid #ddd;
    }

    .comments-section h3 {
      margin-bottom: 1em;
    }

    .post-updated {
      color: #666;
      font-style: italic;
      margin-top: 2em;
    }
  </style>
</head>

<body>
  <div class="left-panel">
    <img src="../pics/noah.png" alt="Noah Dasanaike" width="150">
    <h1>Noah Dasanaike</h1>
    <p>PhD Candidate<br>Department of Government<br>Harvard University</p>
    <a href="../cv/dasanaike_cv.pdf">Curriculum Vitae</a>
  </div>
  <div class="right-panel">
    <hr />
    <p><a href="../index.html">&larr; Back to Home</a> &nbsp;|&nbsp; <a href="../posts.html">All Posts</a></p>

    <div class="bench-title">socOCRbench</div>
    <div class="bench-subtitle">An OCR benchmark for social science documents</div>
    <p class="post-date">February 12, 2026</p>

    <div class="post-content">
      <p>As discussed in a <a href="digitizing-census.html">previous post</a>, existing OCR benchmarks
        are not especially useful for discriminating between models on the kinds of documents that social scientists
        actually work with. Most benchmarks, like OmniDocBench v1.5, over-index on modern printed text, clean scans, and
        well-resourced languages. Handwritten census records, historical logbooks, degraded administrative
        forms, and other ``messy" real-world data are not well represented.</p>

      <p>socOCRbench is a small (private) benchmark designed with this gap in mind. It evaluates OCR models on hundreds
        of samples across two broad task types: handwriting recognition and table extraction. Within each, results are
        split into (A) and (B) sub-categories that roughly correspond to how well-covered the material is by existing
        training data and benchmarks: (A) being more conventional, (B) being more challenging or
        underrepresented. The metric used is Normalized Edit Similarity (NES), where 1.0 represents a perfect
        transcription. Note that details about what data constitutes each category are intentionally unforthcoming to
        protect the sanctity of the evaluation.</p>

      <p>Future iterations of the benchmark will include more diverse document types, larger sample sizes, model
        efficiency, and performance after fine-tuning. </p>

      <table class="results-table sortable">
        <thead>
          <tr>
            <th>Model</th>
            <th>Overall</th>
            <th>HW [A]</th>
            <th>HW [B]</th>
            <th>Tables [A]</th>
            <th>Tables [B]</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td data-sort="Gemini 3 Pro (low)">Gemini 3 Pro (low) <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.5472</td>
            <td>0.6445</td>
            <td class="best">0.5289</td>
            <td>0.8936</td>
            <td>0.2667</td>
          </tr>
          <tr>
            <td data-sort="Gemini 3 Flash (high)">Gemini 3 Flash (high) <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.4808</td>
            <td>0.6049</td>
            <td>0.4750</td>
            <td>0.7551</td>
            <td>0.1862</td>
          </tr>
          <tr>
            <td data-sort="Gemini 2.5 Flash">Gemini 2.5 Flash <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.4596</td>
            <td>0.5390</td>
            <td>0.3689</td>
            <td>0.8734</td>
            <td>0.2541</td>
          </tr>
          <tr>
            <td data-sort="Gemini 2.0 Flash">Gemini 2.0 Flash <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.4286</td>
            <td>0.5849</td>
            <td>0.4312</td>
            <td>0.3748</td>
            <td>0.2454</td>
          </tr>
          <tr>
            <td data-sort="Gemini 3 Flash (low)">Gemini 3 Flash (low) <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td class="best">0.5522</td>
            <td>0.6369</td>
            <td>0.5153</td>
            <td class="best">0.9717</td>
            <td class="best">0.2737</td>
          </tr>
          <tr>
            <td data-sort="Qwen3-VL-235B">Qwen3-VL-235B <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.4428</td>
            <td>0.6174</td>
            <td>0.1969</td>
            <td>0.9398</td>
            <td>0.2520</td>
          </tr>
          <tr>
            <td data-sort="Seed 2.0 Pro">Seed 2.0 Pro <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.4058</td>
            <td>0.4582</td>
            <td>0.2601</td>
            <td>0.9513</td>
            <td>0.2350</td>
          </tr>
          <tr>
            <td data-sort="Qwen3-VL-30B">Qwen3-VL-30B <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.4022</td>
            <td>0.5716</td>
            <td>0.1699</td>
            <td>0.9104</td>
            <td>0.1966</td>
          </tr>
          <tr>
            <td data-sort="GPT-5.2 (low)">GPT-5.2 (low) <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.3759</td>
            <td>0.5140</td>
            <td>0.1693</td>
            <td>0.8445</td>
            <td>0.2014</td>
          </tr>
          <tr>
            <td data-sort="Seed 2.0 Lite">Seed 2.0 Lite <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.3711</td>
            <td>0.4653</td>
            <td>0.1918</td>
            <td>0.9013</td>
            <td>0.1920</td>
          </tr>
          <tr>
            <td data-sort="Datalab (accurate)">Datalab (accurate) <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.3477</td>
            <td>0.3575</td>
            <td>0.1854</td>
            <td>0.9260</td>
            <td>0.2363</td>
          </tr>
          <tr>
            <td data-sort="Gemini 3 Pro (high)">Gemini 3 Pro (high) <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.5406</td>
            <td class="best">0.6589</td>
            <td>0.5106</td>
            <td>0.8989</td>
            <td>0.2400</td>
          </tr>
          <tr>
            <td data-sort="dots.ocr-1.5">dots.ocr-1.5 <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.3720</td>
            <td>0.5670</td>
            <td>0.1423</td>
            <td>0.7768</td>
            <td>0.1810</td>
          </tr>
          <tr>
            <td data-sort="Qwen3-VL-8B">Qwen3-VL-8B <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.3623</td>
            <td>0.5705</td>
            <td>0.1637</td>
            <td>0.8456</td>
            <td>0.0780</td>
          </tr>
          <tr>
            <td data-sort="dots.ocr">dots.ocr <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.3328</td>
            <td>0.4193</td>
            <td>0.0751</td>
            <td>0.9150</td>
            <td>0.2299</td>
          </tr>
          <tr>
            <td data-sort="GLM-OCR">GLM-OCR <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.3247</td>
            <td>0.4369</td>
            <td>0.0693</td>
            <td>0.8200</td>
            <td>0.2283</td>
          </tr>
          <tr>
            <td data-sort="PaddleOCR-VL-1.5">PaddleOCR-VL-1.5 <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.3131</td>
            <td>0.4066</td>
            <td>0.1155</td>
            <td>0.7949</td>
            <td>0.1805</td>
          </tr>
          <tr>
            <td data-sort="GPT-5.2 (high)">GPT-5.2 (high) <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.3932</td>
            <td>0.5190</td>
            <td>0.1959</td>
            <td>0.8951</td>
            <td>0.2074</td>
          </tr>
          <tr>
            <td data-sort="Sarvam Vision">Sarvam Vision <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.2735</td>
            <td>0.2851</td>
            <td>0.1899</td>
            <td>0.7194</td>
            <td>0.1335</td>
          </tr>
          <tr>
            <td data-sort="PP-OCRv5">PP-OCRv5 <span class="model-tag classical">Classical OCR</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.2501</td>
            <td>0.3400</td>
            <td>0.0069</td>
            <td>0.7319</td>
            <td>0.1759</td>
          </tr>
          <tr>
            <td data-sort="Datalab (fast)">Datalab (fast) <span class="model-tag classical">Classical OCR</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.2975</td>
            <td>0.3031</td>
            <td>0.0837</td>
            <td>0.9169</td>
            <td>0.2317</td>
          </tr>
          <tr>
            <td data-sort="PaddleOCR-VL-0.9B">PaddleOCR-VL-0.9B <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.2864</td>
            <td>0.3231</td>
            <td>0.0856</td>
            <td>0.8047</td>
            <td>0.2145</td>
          </tr>
          <tr>
            <td data-sort="Nemotron-Nano-12B">Nemotron-Nano-12B <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.2172</td>
            <td>0.2568</td>
            <td>0.0127</td>
            <td>0.6318</td>
            <td>0.1975</td>
          </tr>
          <tr>
            <td data-sort="OlmOCR-2">OlmOCR-2 <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.2087</td>
            <td>0.3405</td>
            <td>0.0696</td>
            <td>0.2791</td>
            <td>0.1624</td>
          </tr>
          <tr>
            <td data-sort="LightOnOCR-2-1B">LightOnOCR-2-1B <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.2024</td>
            <td>0.3138</td>
            <td>0.1005</td>
            <td>0.4828</td>
            <td>0.0345</td>
          </tr>
          <tr>
            <td data-sort="DeepSeek-OCR2">DeepSeek-OCR2 <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.2039</td>
            <td>0.2202</td>
            <td>0.0637</td>
            <td>0.6022</td>
            <td>0.1480</td>
          </tr>
          <tr>
            <td data-sort="DeepSeek-OCR">DeepSeek-OCR <span class="model-tag vlm">VLM</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.1997</td>
            <td>0.2534</td>
            <td>0.0336</td>
            <td>0.5004</td>
            <td>0.1732</td>
          </tr>
          <tr>
            <td data-sort="Kimi K2.5">Kimi K2.5 <span class="model-tag vlm">VLM</span> <span class="model-tag proprietary">Proprietary</span></td>
            <td>0.1336</td>
            <td>0.2081</td>
            <td>0.0415</td>
            <td>0.3944</td>
            <td>0.0129</td>
          </tr>
          <tr>
            <td data-sort="Tesseract">Tesseract <span class="model-tag classical">Classical OCR</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.1091</td>
            <td>0.1037</td>
            <td>0.0285</td>
            <td>0.1696</td>
            <td>0.1808</td>
          </tr>
          <tr>
            <td data-sort="Layout Parser">Layout Parser <span class="model-tag classical">Classical OCR</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.1082</td>
            <td>0.1037</td>
            <td>0.0285</td>
            <td>0.1609</td>
            <td>0.1815</td>
          </tr>
          <tr>
            <td data-sort="Efficient OCR">EfficientOCR <span class="model-tag classical">Classical OCR</span> <span class="model-tag open-source">Open Source</span></td>
            <td>0.0635</td>
            <td>0.0509</td>
            <td>0.0108</td>
            <td>0.1104</td>
            <td>0.1187</td>
          </tr>
        </tbody>
      </table>

      <details style="margin-top: 2em;">
        <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em;">Methodology &amp; Reproduction</summary>
        <div style="margin-top: 1em; font-size: 0.92em; line-height: 1.6;">

          <p>All VLM models receive the same prompts. For handwriting samples:
            <code>Transcribe all the text in this image exactly as written. Output ONLY the transcribed text, nothing else.</code>
            For table samples:
            <code>OCR this document image into a markdown table. Transcribe all text exactly as written. Output ONLY the markdown table, nothing else.</code>
            Image-only models (dots.ocr, OlmOCR-2) receive no text prompt. dots.ocr-1.5 uses its native
            <code>prompt_ocr</code> prompt (<code>Extract the text content from this image.</code>). The metric is Normalized
            Edit Similarity (NES): <code>1 - edit_distance(pred, gt) / max(len(pred), len(gt))</code>.</p>

          <table class="results-table" style="font-size: 0.9em;">
            <thead>
              <tr>
                <th style="text-align: left;">Model</th>
                <th style="text-align: left;">Provider</th>
                <th style="text-align: left;">Model ID / Notes</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Gemini 3 Pro (low)</td>
                <td>OpenRouter</td>
                <td><code>google/gemini-3-pro-preview</code>, reasoning_effort=low</td>
              </tr>
              <tr>
                <td>Gemini 3 Pro (high)</td>
                <td>OpenRouter</td>
                <td><code>google/gemini-3-pro-preview</code>, reasoning_effort=high</td>
              </tr>
              <tr>
                <td>Gemini 2.5 Flash</td>
                <td>Google GenAI SDK</td>
                <td><code>gemini-2.5-flash</code> via <code>genai.Client</code></td>
              </tr>
              <tr>
                <td>Gemini 2.0 Flash</td>
                <td>OpenRouter</td>
                <td><code>google/gemini-2.0-flash-001</code></td>
              </tr>
              <tr>
                <td>Gemini 3 Flash (low)</td>
                <td>OpenRouter</td>
                <td><code>google/gemini-3-flash-preview</code>, reasoning_effort=low</td>
              </tr>
              <tr>
                <td>Gemini 3 Flash (high)</td>
                <td>OpenRouter</td>
                <td><code>google/gemini-3-flash-preview</code>, reasoning_effort=high</td>
              </tr>
              <tr>
                <td>Qwen3-VL-235B</td>
                <td>OpenRouter</td>
                <td><code>qwen/qwen3-vl-235b-a22b-instruct</code></td>
              </tr>
              <tr>
                <td>Qwen3-VL-30B</td>
                <td>DeepInfra</td>
                <td><code>Qwen/Qwen3-VL-30B-A3B-Instruct</code></td>
              </tr>
              <tr>
                <td>GPT-5.2 (low)</td>
                <td>OpenRouter</td>
                <td><code>openai/gpt-5.2</code>, reasoning_effort=low</td>
              </tr>
              <tr>
                <td>GPT-5.2 (high)</td>
                <td>OpenRouter</td>
                <td><code>openai/gpt-5.2</code>, reasoning_effort=high</td>
              </tr>
              <tr>
                <td>Seed 2.0 Pro</td>
                <td>ZenMux</td>
                <td><code>volcengine/doubao-seed-2.0-pro</code></td>
              </tr>
              <tr>
                <td>Seed 2.0 Lite</td>
                <td>ZenMux</td>
                <td><code>volcengine/doubao-seed-2.0-lite</code></td>
              </tr>
              <tr>
                <td>dots.ocr-1.5</td>
                <td>Local (vLLM)</td>
                <td><code>rednote-hilab/dots.ocr-1.5</code>, native <code>prompt_ocr</code></td>
              </tr>
              <tr>
                <td>dots.ocr</td>
                <td>Replicate</td>
                <td><code>sljeff/dots.ocr</code>, image-only input</td>
              </tr>
              <tr>
                <td>GLM-OCR</td>
                <td>Zhipu AI</td>
                <td><code>glm-ocr</code> via layout parsing API</td>
              </tr>
              <tr>
                <td>PaddleOCR-VL-1.5</td>
                <td>PaddlePaddle Cloud</td>
                <td>Layout parsing API (<code>aistudio-app.com</code>)</td>
              </tr>
              <tr>
                <td>PaddleOCR-VL-0.9B</td>
                <td>PaddlePaddle Cloud</td>
                <td>Layout parsing API (<code>aistudio-app.com</code>)</td>
              </tr>
              <tr>
                <td>Nemotron-Nano-12B</td>
                <td>DeepInfra</td>
                <td><code>nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL</code></td>
              </tr>
              <tr>
                <td>OlmOCR-2</td>
                <td>DeepInfra</td>
                <td><code>allenai/olmOCR-2-7B-1025</code>, image-only input</td>
              </tr>
              <tr>
                <td>LightOnOCR-2-1B</td>
                <td>Local (transformers)</td>
                <td><code>lightonai/LightOnOCR-2-1B</code>, bfloat16, CUDA</td>
              </tr>
              <tr>
                <td>DeepSeek-OCR</td>
                <td>DeepInfra</td>
                <td><code>deepseek-ai/DeepSeek-OCR</code></td>
              </tr>
              <tr>
                <td>DeepSeek-OCR2</td>
                <td>Novita AI</td>
                <td><code>deepseek/deepseek-ocr</code>, native prompts</td>
              </tr>
              <tr>
                <td>Qwen3-VL-8B</td>
                <td>Novita AI</td>
                <td><code>qwen/qwen3-vl-8b-instruct</code></td>
              </tr>
              <tr>
                <td>Sarvam Vision</td>
                <td>Sarvam AI</td>
                <td>Document Intelligence API, output_format=md</td>
              </tr>
              <tr>
                <td>PP-OCRv5</td>
                <td>PaddlePaddle Cloud</td>
                <td>OCR API (<code>aistudio-app.com</code>)</td>
              </tr>
              <tr>
                <td>Tesseract</td>
                <td>Local</td>
                <td>Tesseract 5, <code>eng+fra+deu+nor</code>, psm 6</td>
              </tr>
              <tr>
                <td>Layout Parser</td>
                <td>Local</td>
                <td>Detectron2 (PubLayNet Faster R-CNN) + Tesseract</td>
              </tr>
              <tr>
                <td>EfficientOCR</td>
                <td>Local</td>
                <td>Tiled word-level FAISS KNN recognition</td>
              </tr>
              <tr>
                <td>Kimi K2.5</td>
                <td>OpenRouter</td>
                <td><code>moonshotai/kimi-k2.5</code></td>
              </tr>
              <tr>
                <td>Datalab (fast)</td>
                <td>Datalab API</td>
                <td><code>datalab-python-sdk</code>, mode=fast</td>
              </tr>
              <tr>
                <td>Datalab (accurate)</td>
                <td>Datalab API</td>
                <td><code>datalab-python-sdk</code>, mode=accurate</td>
              </tr>
            </tbody>
          </table>

        </div>
      </details>

      <details style="margin-top: 1.5em;">
        <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em;">Changelog</summary>
        <div style="margin-top: 1em; font-size: 0.92em; line-height: 1.6;">
          <p><strong>February 15, 2026:</strong> Added dots.ocr-1.5, PP-OCRv5, Gemini 2.0 Flash, Kimi K2.5,
            Datalab (fast/accurate), DeepSeek-OCR2, and Qwen3-VL-8B.
            Re-ran Gemini 3 Pro (high), GPT-5.2 (high), and corrected Gemini 3 Flash (low).
            These were affected by a default max_tokens that was too low for thinking models, where reasoning tokens
            count against the output limit, causing many responses to be truncated. After increasing max_tokens to
            64K, Gemini 3 Pro (high) rose from 0.3700 to 0.5406, and GPT-5.2 (high) rose from 0.2841 to 0.3932.
            Gemini 3 Flash (low) scores were also corrected.</p>
        </div>
      </details>

      <p class="post-updated">Last updated: February 15, 2026</p>

    </div>

    <div class="comments-section">
      <h3>Comments</h3>
      <script src="https://giscus.app/client.js" data-repo="noahdasanaike/noahdasanaike.github.io"
        data-repo-id="R_kgDOG0UbfQ" data-category="General" data-category-id="DIC_kwDOG0Ubfc4C17dM"
        data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0"
        data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="en" crossorigin="anonymous" async>
        </script>
    </div>
  </div>

  <script>
    (function () {
      function getCellText(cell) {
        if (!cell) return "";
        if (cell.dataset && typeof cell.dataset.sort === "string") return cell.dataset.sort;
        return String(cell.innerText || cell.textContent || "").trim();
      }

      function parseNumber(text) {
        var t = String(text || "").replace(/,/g, "").replace(/%/g, "").trim();
        if (t === "") return null;
        if (!/^-?\d+(\.\d+)?$/.test(t)) return null;
        var n = Number(t);
        return isFinite(n) ? n : null;
      }

      function stringCompare(a, b) {
        return String(a).localeCompare(String(b), undefined, { numeric: true, sensitivity: "base" });
      }

      function sortTableByColumn(table, columnIndex, dir) {
        var tbody = table.tBodies && table.tBodies[0];
        if (!tbody) return;

        var rows = Array.prototype.slice.call(tbody.querySelectorAll("tr"));
        var wrapped = rows.map(function (tr, idx) { return { tr: tr, idx: idx }; });

        wrapped.sort(function (ra, rb) {
          var aCell = ra.tr.children && ra.tr.children[columnIndex];
          var bCell = rb.tr.children && rb.tr.children[columnIndex];
          var aText = getCellText(aCell);
          var bText = getCellText(bCell);

          var aNum = parseNumber(aText);
          var bNum = parseNumber(bText);

          var cmp = 0;
          if (aNum !== null && bNum !== null) {
            cmp = aNum === bNum ? 0 : (aNum < bNum ? -1 : 1);
          } else {
            cmp = stringCompare(aText, bText);
          }

          if (cmp === 0) cmp = ra.idx - rb.idx; // stable sort
          return dir === "asc" ? cmp : -cmp;
        });

        wrapped.forEach(function (r) { tbody.appendChild(r.tr); });
      }

      function clearSortIndicators(headers) {
        Array.prototype.forEach.call(headers, function (th) {
          th.classList.remove("sort-asc", "sort-desc");
          th.removeAttribute("aria-sort");
        });
      }

      document.addEventListener("DOMContentLoaded", function () {
        var tables = document.querySelectorAll("table.sortable");
        Array.prototype.forEach.call(tables, function (table) {
          var headers = table.querySelectorAll("thead th");
          if (!headers.length) return;

          Array.prototype.forEach.call(headers, function (th, idx) {
            th.tabIndex = 0;
            th.title = "Click to sort";

            function handleActivate() {
              var currentCol = table.getAttribute("data-sort-col");
              var currentDir = table.getAttribute("data-sort-dir") || "asc";

              var nextDir = "asc";
              if (String(currentCol) === String(idx)) {
                nextDir = currentDir === "asc" ? "desc" : "asc";
              } else {
                // Default to descending for numeric columns (common for leaderboard tables).
                var tbody = table.tBodies && table.tBodies[0];
                if (tbody) {
                  var trs = tbody.querySelectorAll("tr");
                  for (var i = 0; i < trs.length && i < 10; i++) {
                    var cell = trs[i].children && trs[i].children[idx];
                    var txt = getCellText(cell);
                    if (txt !== "") {
                      nextDir = parseNumber(txt) !== null ? "desc" : "asc";
                      break;
                    }
                  }
                }
              }

              table.setAttribute("data-sort-col", String(idx));
              table.setAttribute("data-sort-dir", nextDir);

              clearSortIndicators(headers);
              th.classList.add(nextDir === "asc" ? "sort-asc" : "sort-desc");
              th.setAttribute("aria-sort", nextDir === "asc" ? "ascending" : "descending");

              sortTableByColumn(table, idx, nextDir);
            }

            th.addEventListener("click", handleActivate);
            th.addEventListener("keydown", function (e) {
              if (e.key === "Enter" || e.key === " ") {
                e.preventDefault();
                handleActivate();
              }
            });
          });
          // Default sort: Overall (column 1) descending
          var overallIdx = 1;
          table.setAttribute("data-sort-col", String(overallIdx));
          table.setAttribute("data-sort-dir", "desc");
          headers[overallIdx].classList.add("sort-desc");
          headers[overallIdx].setAttribute("aria-sort", "descending");
          sortTableByColumn(table, overallIdx, "desc");
        });
      });
    })();
  </script>
</body>

</html>